import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pathlib import Path

import hydra
import lightning as L
import torch
from omegaconf import DictConfig
from PIL import Image
from torchvision import transforms

from rps.data import RPSDataModule
from rps.module import RPSModule

# –ö–ª–∞—Å—Å—ã RPS –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
CLASS_NAMES = ["rock", "paper", "scissors"]


@hydra.main(version_base=None, config_path="../conf", config_name="config")
def infer(cfg: DictConfig):
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö...")

    # === BATCH –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ===
    datamodule = RPSDataModule(
        cfg.data.train_data_dir,
        cfg.data.test_data_dir,
        cfg.data.val_data_dir,
        cfg.data.train_batch_size,
        cfg.data.test_batch_size,
    )

    # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ó–∞–≥—Ä—É–∂–∞–µ–º –í –ü–†–Ø–ú–û–ô –ú–û–î–£–õ–¨, –∞ –Ω–µ module.model
    module = RPSModule(num_classes=cfg.model.num_classes)
    module.load_state_dict(torch.load(cfg.output_file, weights_only=True))  # ‚Üê –ó–î–ï–°–¨!
    module.eval()

    trainer = L.Trainer(
        accelerator=cfg.trainer.accelerator,
        devices=cfg.trainer.devices,
        precision=16,
    )

    trainer.test(module, datamodule=datamodule)
    print("‚úì Batch —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω")

    # === –û–î–ò–ù–û–ß–ù–´–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø ===
    single_predict(module, cfg)
    return module


def single_predict(module, cfg: DictConfig):
    transform = transforms.Compose(
        [
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ]
    )

    test_images = ["test_rock.jpg", "test_paper.jpg", "test_scissors.jpg"]
    CLASS_NAMES = ["rock", "paper", "scissors"]

    module.eval()
    device = next(module.parameters()).device

    for img_path in test_images:
        if Path(img_path).exists():
            image = Image.open(img_path).convert("RGB")
            img_tensor = transform(image).unsqueeze(0).to(device)

            with torch.no_grad():
                logits = module(img_tensor)  # ‚Üê –ü–†–Ø–ú–û module!
                probs = torch.softmax(logits, dim=1)
                pred_class = torch.argmax(probs, dim=1).item()

            print(f"üñºÔ∏è {img_path}: {CLASS_NAMES[pred_class]} ({probs.max():.1%})")


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Å –∫–æ–Ω—Ñ–∏–≥–æ–º (–∫–∞–∫ –≤ train)
    model = infer()

    print("\nüéâ Inference –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print("–î–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ–ª–æ–∂–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç–µ–∫—É—â—É—é –ø–∞–ø–∫—É")
    print("–§–∞–π–ª—ã: test_rock.jpg, test_paper.jpg, test_scissors.jpg, my_photo.jpg")
